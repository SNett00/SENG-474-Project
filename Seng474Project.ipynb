{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "Xd1XGIwwB7Lg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jams in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (0.3.4)\n",
      "Requirement already satisfied: mir-eval>=0.5 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (0.7)\n",
      "Requirement already satisfied: pandas in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (1.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (1.22.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (2.4.0)\n",
      "Requirement already satisfied: decorator in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (5.1.1)\n",
      "Requirement already satisfied: six in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jams) (1.16.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jsonschema>=3.0.0->jams) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jsonschema>=3.0.0->jams) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from jsonschema>=3.0.0->jams) (21.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.0->jams) (3.7.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from mir-eval>=0.5->jams) (1.7.3)\n",
      "Requirement already satisfied: future in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from mir-eval>=0.5->jams) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from pandas->jams) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from pandas->jams) (2.8.2)\n",
      "Requirement already satisfied: matplotlib in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (1.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: audiolazy in /Users/shawnnettleton/opt/miniconda3/envs/apress/lib/python3.8/site-packages (0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install jams\n",
    "!pip install matplotlib\n",
    "!pip install audiolazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "pwshn6LfKd5X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import jams\n",
    "import audiolazy as al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQE8nV4nh1f2"
   },
   "source": [
    "## Process input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "Zn8-Wa-XEcV9"
   },
   "outputs": [],
   "source": [
    "def timeKey(t):\n",
    "    return t[0]\n",
    "\n",
    "def loadNoteData(f):\n",
    "    #f = open('/content/JamsFiles/04_Rock1-90-C_solo.jams', 'r')\n",
    "\n",
    "   # data = f.read()\n",
    "    data = jams.load(f)\n",
    "    notes = []\n",
    "\n",
    "    for i in range (0, 6):\n",
    "        for j in data.annotations[\"note_midi\"][i][\"data\"]:\n",
    "            notes.append([j[0],j[1],j[2]])\n",
    "            \n",
    "    notes.sort(key=timeKey)\n",
    "\n",
    "    #pprint.pprint(notes)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#function to process the raw audio data.\n",
    "def x_data_process(raw_frame):\n",
    "    return raw_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "iBBc63iIocsB"
   },
   "outputs": [],
   "source": [
    "#iterate through each song, one frame at a time based off hop and window size\n",
    "def songProcess(audio, notes, sr, hopSize = 1024, winSize = 4096, features = [], labels = []):\n",
    "\n",
    "    testCount = 0\n",
    "\n",
    "    offsets = np.arange(0,len(audio),hopSize)\n",
    "    for (i,o) in enumerate(offsets):\n",
    "        testCount += 1\n",
    "\n",
    "        frame = audio[o:o+winSize]\n",
    "        note = 0\n",
    "        tiebreak = []\n",
    "\n",
    "        #Create labels from jams file data -- if multiple notes in frame, take longest duration\n",
    "        for j,(time,duration,value) in enumerate(notes):\n",
    "            note_start = time*sr\n",
    "            note_end = (time+duration)*sr\n",
    "            if o <= note_start < o+winSize:                 #note starts in frame\n",
    "                tiebreak.append(j)\n",
    "            elif o <= note_end < o+winSize:                  #note ends in frame\n",
    "                tiebreak.append(j)\n",
    "            elif note_start < o and o+winSize <= note_end: #note continuous thru frame\n",
    "                note = value\n",
    "\n",
    "        #if multiple notes in frame, choose one that played the longest in frame\n",
    "        if len(tiebreak) > 0:\n",
    "            if len(tiebreak) == 1:\n",
    "                note = notes[tiebreak[0]][2]\n",
    "            else:\n",
    "                max_dur = 0\n",
    "                max_note = 0\n",
    "                for index in tiebreak:\n",
    "                    note_start = notes[index][0] * sr\n",
    "                    note_duration = notes[index][1] * sr\n",
    "                    frame_dur = 0\n",
    "                    if note_start < o:\n",
    "                        frame_dur = note_duration+note_start-o\n",
    "                    elif note_start+note_duration > o+winSize:\n",
    "                        frame_dur = o+winSize - note_start\n",
    "                    else:\n",
    "                        frame_dur = note_duration\n",
    "\n",
    "                    if frame_dur > max_dur:\n",
    "                        max_dur = frame_dur\n",
    "                        max_note = notes[index][2]\n",
    "                note = max_note\n",
    "\n",
    "\n",
    "        #pad feature matrix\n",
    "        if len(frame) < winSize:\n",
    "            pad = winSize-len(frame)\n",
    "            temp = np.zeros(pad)\n",
    "            frame = np.concatenate((frame,temp))\n",
    "        features.append(x_data_process(frame))    #process raw frame... somehow??\n",
    "        labels.append(round(note))  #quantize to the nearest midi value\n",
    "\n",
    "\n",
    "    return features,labels,testCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IULUskjWZhR"
   },
   "source": [
    "## Data classification using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "zQ3YbkqhXLv9"
   },
   "outputs": [],
   "source": [
    "def neuralNetwork(XTrain, XTest, YTrain, YTest):\n",
    "    #Currently, using successful values from A1, further adjustment with the processed dataset will be needed\n",
    "    MLPC = MLPClassifier(hidden_layer_sizes=(71, 21), max_iter=10000, alpha=0.0001, learning_rate_init=0.001, solver='adam')\n",
    "    MLPC.fit(XTrain, YTrain)\n",
    "\n",
    "    return accuracy_score(YTest, MLPC.predict(XTest)), MLPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMmySWxcgwn9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ba6L3QgXODZ"
   },
   "source": [
    "## Data classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "A1pTsklKXPcL"
   },
   "outputs": [],
   "source": [
    "def gaussianSVM(XTrain, XTest, YTrain, YTest):\n",
    "    #Currently, using successful values from A2, further adjustment with the processed dataset will be needed\n",
    "    svc = SVC(kernel = 'rbf', gamma = 0.1, C = 10)\n",
    "    svc.fit(XTrain, YTrain)\n",
    "\n",
    "    return accuracy_score(YTest, svc.predict(XTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "BmNXFTYPgFBL"
   },
   "outputs": [],
   "source": [
    "def linearSVM(XTrain, XTest, YTrain, YTest):\n",
    "    #Currently, using successful values from A2, further adjustment with the processed dataset will be needed\n",
    "    svc = LinearSVC(C = 0.0225)\n",
    "\n",
    "    svc.fit(XTrain, YTrain)\n",
    "    accuracy_score(YTest, svc.predict(XTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Taking the song path, trained model, and same hopSize and winSize as used in training,\n",
    "Converts song into predicted midi information\"\"\"\n",
    "def predict(song_path, model, hopSize, winSize):\n",
    "\n",
    "    sample_rate, audio = wav.read(song_path)\n",
    "    offsets = np.arange(0, len(audio), hopSize)\n",
    "    x_data = []\n",
    "    \n",
    "    for (i,o) in enumerate(offsets):\n",
    "        frame = audio[o:o+winSize]\n",
    "        frame = x_data_process(frame)\n",
    "        #padint\n",
    "        if len(frame) < winSize:\n",
    "            pad = winSize-len(frame)\n",
    "            temp = np.zeros(pad)\n",
    "            frame = np.concatenate((frame,temp))\n",
    "        x_data.append(frame)\n",
    "\n",
    "    #column 0 is note, column 1 is time\n",
    "    midi_info = np.zeros((len(offsets), 2))\n",
    "    raw_results = model.predict(x_data) \n",
    "    \n",
    "    for i, frame_note in enumerate(raw_results):\n",
    "        midi_info[i][0] = frame_note\n",
    "        midi_info[i][1] = hopSize*i/sample_rate\n",
    "        \n",
    "    \n",
    "    # TODO: create proper midi file from this note / time information\n",
    "    \n",
    "    \n",
    "    # Also determine method to covert midi into sheet music\n",
    "    \n",
    "\n",
    "    return midi_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8DfjYyNg0NZ"
   },
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "CkZh6bSmKhw3"
   },
   "outputs": [],
   "source": [
    "# LOAD SONG AND JAM FILES\n",
    "song_path = r'DataSets/audio_mono-mic'\n",
    "jam_path = r'DataSets/annotation'\n",
    "MODE = 'solo'\n",
    "inputFiles = list(zip([x for x in os.listdir(song_path) if MODE in x],[x for x in os.listdir(jam_path) if MODE in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "XData,YData = [],[]\n",
    "\n",
    "numSongs = 5\n",
    "counter = numSongs\n",
    "\n",
    "notecounter = 0\n",
    "\n",
    "hopSize = 1024\n",
    "winSize = 1024\n",
    "\n",
    "\n",
    "for song_file, jam_file in inputFiles:\n",
    "    song = os.path.join(song_path, song_file)\n",
    "    jam = os.path.join(jam_path, jam_file)\n",
    "\n",
    "    sampleRate, audio = wav.read(song)\n",
    "    note_info = loadNoteData(jam)\n",
    "    #NOTE: window size used in previous works with dataset where 0.2s\n",
    "    XData, YData, notecount = songProcess(audio, note_info, sampleRate, hopSize=hopSize, winSize = winSize) #default window and hop. Can pass in a feature matrix and label array if we want to concat multiple songs together\n",
    "    notecounter += notecount\n",
    "    counter -= 1\n",
    "    \n",
    "    if counter == 0:\n",
    "        break\n",
    "\n",
    "\n",
    "#Split data\n",
    "XTrain, XTest, YTrain, YTest = train_test_split(XData, YData, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 song neural network accuracy: 0.27943570265870865\n"
     ]
    }
   ],
   "source": [
    "#Call ML functions..\n",
    "# gaussianSVM(XTrain,YTrain)\n",
    "# linearSVM(XTrain,YTrain)\n",
    "song_result, trained_model = neuralNetwork(XTrain, XTest, YTrain, YTest)\n",
    "print(str(numSongs)+\" song neural network accuracy:\", song_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Call predict function\n",
    "# song = 'DataSets/audio_mono-mic/00_BN3-119-G_solo_mic.wav'\n",
    "\n",
    "ssong = 'DataSets/audio_mono-mic/04_Rock1-90-C#_solo_mic.wav'\n",
    "\n",
    "midi_info = predict(song, trained_model, hopSize, winSize)\n",
    "\n",
    "# for note, time in midi_info:\n",
    "#     print(note,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNV465-nwiWH"
   },
   "source": [
    "## Testing Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YZJnOBjgghz7"
   },
   "outputs": [],
   "source": [
    "f = open('/content/JamsFiles/04_Rock1-90-C_solo.jams', 'r')\n",
    "\n",
    "# data = f.read()\n",
    "data = jams.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gOys91nXrHn",
    "outputId": "bcb333e8-0b4d-4e9a-cfab-4b690efa9a2b"
   },
   "outputs": [],
   "source": [
    "print(\"\\ttime\\tduration\\tnotes\")\n",
    "pprint.pprint(loadNoteData(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "pFxIOfooXu4X"
   },
   "outputs": [],
   "source": [
    "#File loading Test\n",
    "for song,jam in inputFiles:\n",
    "    if song[:-13] != jam[:-10]:\n",
    "        print(\"error with:\",song,jam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test XData padding\n",
    "for x in XData:\n",
    "    if len(x) < 4096:\n",
    "        print(\"Error with Xdata padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Check to see if XData and YData arrays are being appended properly\n",
    "if notecounter != len(XData):\n",
    "    print(\"Error: XData and YData arrays are not being apppended properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI \tNote  \tFreq \t\t\tTime\n",
      "50.0 \t D3 \t 146.8323839587038 \t 1.8575963718820863\n",
      "60.0 \t C4 \t 261.6255653005986 \t 1.9504761904761905\n",
      "60.0 \t C4 \t 261.6255653005986 \t 3.9706122448979593\n",
      "50.0 \t D3 \t 146.8323839587038 \t 5.828208616780046\n",
      "60.0 \t C4 \t 261.6255653005986 \t 7.894784580498866\n",
      "60.0 \t C4 \t 261.6255653005986 \t 20.456780045351476\n",
      "60.0 \t C4 \t 261.6255653005986 \t 21.91963718820862\n",
      "69.0 \t A4 \t 440.0 \t 22.709115646258503\n"
     ]
    }
   ],
   "source": [
    "# From the predict() function above to get the note and time info we can see what note and freq it represents\n",
    "\n",
    "# Reference: https://pythonhosted.org/audiolazy/lazy_midi.html\n",
    "\n",
    "print('MIDI \\tNote  \\tFreq \\t\\t\\tTime')\n",
    "for note, time in midi_info:\n",
    "    if note != 0.0:\n",
    "        print(note,'\\t',al.midi2str(note),'\\t',al.midi2freq(note),'\\t',time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seng474Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
